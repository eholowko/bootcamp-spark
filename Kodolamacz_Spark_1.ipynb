{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APACHE SPARK\n",
    "\n",
    "### Dzień 1\n",
    "\n",
    "#### Wprowadzenie + Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Spark to silnik do obliczeń rozproszonych na licencji open-source. Pierwotnie powstał na Berkley, po czym przekazano go do Apache Software Foundation gdzie jest od tamtej pory utrzymywany i rozwijany. Spark oferuje interfejs pozwalający na programowanie obliczeń na klastrach z domyślną paralelizacją oraz odpornością na awarie.\n",
    "\n",
    "Spark dostępny jest w Scali, Pythonie, Javie oraz R.\n",
    "\n",
    "**Komponenty Sparka:**\n",
    "* Spark core - podstawa Sparka z podstawową abstrakcją danych nazywaną RDD\n",
    "* Spark SQL - komponent pozwalający na operowanie na ustrukturyzowanych danych z wykorzystaniem operacji znanych z SQL - łatwy w użyciu\n",
    "* Spark MLlib - komponent zawierający algorytmy ML dostępne w Sparku - ML na skalę klastrów\n",
    "* Spark Streaming - moduł pozwalający na pracę ze strumnieniami danych\n",
    "* Spark GraphX - komponent do pracy z grafami\n",
    "\n",
    "**Architektura Sparka:**\n",
    "* driver - proces uruchamiający główną funkcję aplikacji i tworzący SparkContext\n",
    "* executor(y) - proces uruchomiony dla aplikacji w węźle roboczym (worker node), który uruchamia zadania i przechowuje dane w pamięci lub na dysku. Każda aplikacja ma własne executory\n",
    "* cluster manager - dostępne opcje: YARN, Mesos, Kubernetes, Standalone\n",
    "\n",
    "**SparkContext:**\n",
    "* punkt wejścia do pracy ze Sparkiem\n",
    "* koordynuje procesy na klastrze\n",
    "* zatrzymanie SparkContextu == zatrzymanie działania aplikacji\n",
    "* zwykle nazywany `sc`\n",
    "* kroki niezbędne do utworzenia SparkContextu w pySparku:\n",
    "\n",
    "> import pyspark\n",
    "\n",
    "> sc = pyspark.SparkContext(appName=\"my_app\")\n",
    "\n",
    "**SparkSession:**\n",
    "* wprowadzony w Spark 2.0\n",
    "* składa się ze SparkContextu, SQLContextu oraz HiveContext\n",
    "* zwykle nazywany `spark`\n",
    "* kroki niezbędne do utworzenia SparkSession w pySparku:\n",
    "\n",
    "> from pyspark.sql import SparkSession\n",
    "\n",
    "> spark = SparkSession.builder.appName('my_app').getOrCreate()\n",
    "\n",
    "\n",
    "**RDD:**\n",
    "* podstawowa abstrakcja danych w Sparku\n",
    "* R - resilient\n",
    "* D - distributed\n",
    "* D - dataset\n",
    "* Matei Zharia, et al. `Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing`\n",
    "* immutable\n",
    "* in-memory\n",
    "* lazy evaluated\n",
    "* parallel\n",
    "* dwa typy operacji: akcje i transformacje\n",
    "* przykłady użycia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName=\"my_app\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.context.SparkContext"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "378"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize(range(20)) \\\n",
    ".map(lambda x: x * 2) \\\n",
    ".filter(lambda x: x != 2) \\\n",
    ".reduce(lambda x,y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize(range(20)) \\\n",
    ".map(lambda x: x * 2) \\\n",
    ".filter(lambda x: x != 2).collect()# pobranie elementów do drivera, uwaga przy dużych RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('is', 2),\n",
       " ('engine', 1),\n",
       " ('compatible', 1),\n",
       " ('hadoop', 3),\n",
       " ('run', 1),\n",
       " ('in', 2),\n",
       " ('clusters', 1),\n",
       " ('yarn', 1),\n",
       " (\"spark's\", 1),\n",
       " ('mode', 1),\n",
       " ('process', 1),\n",
       " ('hdfs', 1),\n",
       " ('cassandra', 1),\n",
       " ('hive', 1),\n",
       " ('designed', 1),\n",
       " ('perform', 1),\n",
       " ('both', 1),\n",
       " ('similar', 1),\n",
       " ('new', 1),\n",
       " ('like', 1),\n",
       " ('streaming', 1),\n",
       " ('machine', 1),\n",
       " ('learning', 1),\n",
       " ('spark', 1),\n",
       " ('a', 1),\n",
       " ('fast', 1),\n",
       " ('and', 5),\n",
       " ('general', 1),\n",
       " ('processing', 2),\n",
       " ('with', 1),\n",
       " ('data', 2),\n",
       " ('it', 3),\n",
       " ('can', 2),\n",
       " ('through', 1),\n",
       " ('or', 1),\n",
       " ('standalone', 1),\n",
       " ('hbase', 1),\n",
       " ('any', 1),\n",
       " ('inputformat', 1),\n",
       " ('to', 2),\n",
       " ('batch', 1),\n",
       " ('mapreduce', 1),\n",
       " ('workloads', 1),\n",
       " ('interactive', 1),\n",
       " ('queries', 1)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exercise_input = \"\"\"Spark is a fast and general processing engine compatible with Hadoop data. \\\n",
    "It can run in Hadoop clusters through YARN or Spark's standalone mode, and it can process data in HDFS, \\\n",
    "HBase, Cassandra, Hive, and any Hadoop InputFormat. It is designed to perform both batch processing \\\n",
    "(similar to MapReduce) and new workloads like streaming, interactive queries, and machine learning.\"\"\"\n",
    "\n",
    "sc.parallelize(exercise_input.split(\" \")) \\\n",
    ".map(lambda x: x.lower().strip(\",.()\")) \\\n",
    ".map(lambda x: (x, 1)) \\\n",
    ".reduceByKey(lambda x,y: x + y) \\\n",
    ".collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DataFrame:**\n",
    "* abstrakcja danych z modułu Spark SQL - u podstaw leżą RDD\n",
    "* immutable\n",
    "* in-memory\n",
    "* resilient\n",
    "* distributed\n",
    "* parallel\n",
    "* przechowuje dodatkowe informacje o strukturze danych (schema)\n",
    "* rozproszona kolekcja wierszy z nazwanymi kolumnami\n",
    "* optymalizowane przez Catalyst Optymizer\n",
    "* pozwala na pracę z danymi wykorzysując zapytania znane z SQL/Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions as f# importujemy tak, żeby nie nadpisać funkcji pythonowych\n",
    "\n",
    "spark = SparkSession.builder.appName('my_app').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DataFrame - kolekcja wierszy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_df = spark.createDataFrame([Row(name='Greg', age=32),\n",
    "                                 Row(name='Bob', age=27),\n",
    "                                 Row(name='Alice', age=30)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age| name|\n",
      "+---+-----+\n",
      "| 32| Greg|\n",
      "| 27|  Bob|\n",
      "| 30|Alice|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dummy_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_df2 = spark.createDataFrame([Row(name='Bill', age=26),\n",
    "                                  Row(name='Carol', age=28),\n",
    "                                  Row(name='Susan', age=25)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age| name|\n",
      "+---+-----+\n",
      "| 26| Bill|\n",
      "| 28|Carol|\n",
      "| 25|Susan|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dummy_df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=32, name='Greg'), Row(age=27, name='Bob'), Row(age=30, name='Alice')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_df.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DataFrame - właściwości**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age', 'name']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('age', 'bigint'), ('name', 'string')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dummy_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----+\n",
      "|summary|               age| name|\n",
      "+-------+------------------+-----+\n",
      "|  count|                 3|    3|\n",
      "|   mean|29.666666666666668| null|\n",
      "| stddev|2.5166114784235836| null|\n",
      "|    min|                27|Alice|\n",
      "|    max|                32| Greg|\n",
      "+-------+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dummy_df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sposoby odwoływania się do kolumn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'age'>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_df.age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'age'>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_df[\"age\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'age'>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_df[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Składnia inspirowana SQL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|age|\n",
      "+---+\n",
      "| 32|\n",
      "| 27|\n",
      "| 30|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dummy_df.select(\"age\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age| name|\n",
      "+---+-----+\n",
      "| 32| Greg|\n",
      "| 30|Alice|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dummy_df.where(dummy_df[\"age\"] > 27).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|          avg(age)|\n",
      "+------------------+\n",
      "|29.666666666666668|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dummy_df.agg(f.avg(dummy_df[\"age\"])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|          avg(age)|\n",
      "+------------------+\n",
      "|29.666666666666668|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lub bardziej SQLowo\n",
    "dummy_df.select(f.avg(dummy_df[\"age\"])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age| name|\n",
      "+---+-----+\n",
      "| 32| Greg|\n",
      "| 27|  Bob|\n",
      "| 30|Alice|\n",
      "| 26| Bill|\n",
      "| 28|Carol|\n",
      "| 25|Susan|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dummy_df.union(dummy_df2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zapisywanie wyników**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dummy_df.write.parquet('path/to/location/dummy_df.parquet')\n",
    "#dummy_df.write.csv('path/to/location/dummy_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wczytywanie danych**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#parquet_df = spark.read.parquet('path/to/parquet')\n",
    "#csv_df = spark.read.csv('path/to/csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Używanie zapytań SQLowych**\n",
    "\n",
    "https://spark.apache.org/docs/latest/sql-programming-guide.html#compatibility-with-apache-hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_df.createOrReplaceTempView('dummy_df')# zarejestrowanie df jako TempView"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age| name|\n",
      "+---+-----+\n",
      "| 32| Greg|\n",
      "| 27|  Bob|\n",
      "| 30|Alice|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# query zwraca nowy DataFrame\n",
    "spark.sql('select * from dummy_df').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.dropTempView(\"dummy_df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przykładowe transformacje z wykorzystaniem API Spark SQL oraz selectów SQLowych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tworzenie pokazowych DataFrameów**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_id = [random.choice([\"regA\",\"regB\",\"regC\",\"regD\",\"regE\",\"regF\"]) for x in range(400)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_id = [random.choice([\"prodA\",\"prodB\",\"prodC\",\"prodD\",\"prodE\",\"prodF\",\n",
    "                          \"prodG\",\"prodH\",\"prodI\",\"prodJ\",\"prodK\",\"prodL\",\"prodM\"]) for x in range(400)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = [random.uniform(1000,10000) for x in range(400)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([Row(prod=p, geo=g, val=v) for p,g,v in zip(prod_id, geo_id, value)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------------------+\n",
      "| geo| prod|               val|\n",
      "+----+-----+------------------+\n",
      "|regF|prodB| 3941.612243931777|\n",
      "|regA|prodD| 7851.067371046244|\n",
      "|regA|prodF| 4412.135940077227|\n",
      "|regF|prodE| 7768.088411993063|\n",
      "|regC|prodC| 8487.318566397455|\n",
      "|regB|prodH|3270.4437860414255|\n",
      "|regB|prodI| 1737.156094854783|\n",
      "|regB|prodL|1174.4495834500963|\n",
      "|regF|prodE| 5854.771431302804|\n",
      "|regA|prodJ| 9999.170456582882|\n",
      "|regF|prodM| 4149.643093481655|\n",
      "|regF|prodK| 6851.296839248875|\n",
      "|regE|prodI|8031.0974464980545|\n",
      "|regA|prodA| 6865.791897195005|\n",
      "|regE|prodK| 7788.098836535736|\n",
      "|regD|prodI|   9546.5055944439|\n",
      "|regA|prodE|2794.2461412627963|\n",
      "|regA|prodK|1183.4201558829911|\n",
      "|regA|prodB|2371.4411120631135|\n",
      "|regB|prodC|2135.9887738678563|\n",
      "+----+-----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('train_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_df = spark.createDataFrame([Row(geo_id = \"regA\", geo_name = \"Region A\"),\n",
    "                                Row(geo_id = \"regB\", geo_name = \"Region B\"),\n",
    "                                Row(geo_id = \"regC\", geo_name = \"Region C\"),\n",
    "                                Row(geo_id = \"regD\", geo_name = \"Region D\"),\n",
    "                                Row(geo_id = \"regE\", geo_name = \"Region_E\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|geo_id|geo_name|\n",
      "+------+--------+\n",
      "|  regA|Region A|\n",
      "|  regB|Region B|\n",
      "|  regC|Region C|\n",
      "|  regD|Region D|\n",
      "|  regE|Region_E|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "geo_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_df.createOrReplaceTempView(\"geo_df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "| prod|\n",
      "+-----+\n",
      "|prodB|\n",
      "|prodD|\n",
      "|prodF|\n",
      "|prodE|\n",
      "|prodC|\n",
      "+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"prod\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|lower(prod)|\n",
      "+-----------+\n",
      "|      prodb|\n",
      "|      prodd|\n",
      "|      prodf|\n",
      "|      prode|\n",
      "|      prodc|\n",
      "|      prodh|\n",
      "|      prodi|\n",
      "|      prodl|\n",
      "|      prode|\n",
      "|      prodj|\n",
      "|      prodm|\n",
      "|      prodk|\n",
      "|      prodi|\n",
      "|      proda|\n",
      "|      prodk|\n",
      "|      prodi|\n",
      "|      prode|\n",
      "|      prodk|\n",
      "|      prodb|\n",
      "|      prodc|\n",
      "+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(f.lower(f.col('prod'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "| prod|\n",
      "+-----+\n",
      "|prodB|\n",
      "|prodD|\n",
      "|prodF|\n",
      "|prodE|\n",
      "|prodC|\n",
      "+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select prod from train_df\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "| geo|               val|\n",
      "+----+------------------+\n",
      "|regF| 3941.612243931777|\n",
      "|regA| 7851.067371046244|\n",
      "|regA| 4412.135940077227|\n",
      "|regF| 7768.088411993063|\n",
      "|regC| 8487.318566397455|\n",
      "|regB|3270.4437860414255|\n",
      "|regB| 1737.156094854783|\n",
      "|regB|1174.4495834500963|\n",
      "|regF| 5854.771431302804|\n",
      "|regA| 9999.170456582882|\n",
      "|regF| 4149.643093481655|\n",
      "|regF| 6851.296839248875|\n",
      "|regE|8031.0974464980545|\n",
      "|regA| 6865.791897195005|\n",
      "|regE| 7788.098836535736|\n",
      "|regD|   9546.5055944439|\n",
      "|regA|2794.2461412627963|\n",
      "|regA|1183.4201558829911|\n",
      "|regA|2371.4411120631135|\n",
      "|regB|2135.9887738678563|\n",
      "+----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.drop(\"prod\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "| geo|               val|\n",
      "+----+------------------+\n",
      "|regF| 3941.612243931777|\n",
      "|regA| 7851.067371046244|\n",
      "|regA| 4412.135940077227|\n",
      "|regF| 7768.088411993063|\n",
      "|regC| 8487.318566397455|\n",
      "|regB|3270.4437860414255|\n",
      "|regB| 1737.156094854783|\n",
      "|regB|1174.4495834500963|\n",
      "|regF| 5854.771431302804|\n",
      "|regA| 9999.170456582882|\n",
      "|regF| 4149.643093481655|\n",
      "|regF| 6851.296839248875|\n",
      "|regE|8031.0974464980545|\n",
      "|regA| 6865.791897195005|\n",
      "|regE| 7788.098836535736|\n",
      "|regD|   9546.5055944439|\n",
      "|regA|2794.2461412627963|\n",
      "|regA|1183.4201558829911|\n",
      "|regA|2371.4411120631135|\n",
      "|regB|2135.9887738678563|\n",
      "+----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select geo, val from train_df\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Group by**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "| prod|          sum(val)|\n",
      "+-----+------------------+\n",
      "|prodE|200344.20973732683|\n",
      "|prodM|208910.57198616146|\n",
      "|prodL|180375.40537054365|\n",
      "|prodI|195882.05155923506|\n",
      "|prodB|118974.31060099883|\n",
      "|prodG|204282.66023881637|\n",
      "|prodC| 185378.1188498701|\n",
      "|prodJ|153095.65225463244|\n",
      "|prodA|165984.72898796143|\n",
      "|prodD|180665.70024676438|\n",
      "|prodF| 144171.5337693601|\n",
      "|prodK|184605.15827039533|\n",
      "|prodH|152223.36908939172|\n",
      "+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"prod\").sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "| prod|          sum(val)|\n",
      "+-----+------------------+\n",
      "|prodE|200344.20973732683|\n",
      "|prodM|208910.57198616146|\n",
      "|prodL|180375.40537054365|\n",
      "|prodI|195882.05155923506|\n",
      "|prodB|118974.31060099883|\n",
      "|prodG|204282.66023881637|\n",
      "|prodC| 185378.1188498701|\n",
      "|prodJ|153095.65225463244|\n",
      "|prodA|165984.72898796143|\n",
      "|prodD|180665.70024676438|\n",
      "|prodF| 144171.5337693601|\n",
      "|prodK|184605.15827039533|\n",
      "|prodH|152223.36908939172|\n",
      "+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select prod, sum(val) from train_df group by prod\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------------------+\n",
      "| prod| geo|          sum(val)|\n",
      "+-----+----+------------------+\n",
      "|prodI|regB| 26614.21261323439|\n",
      "|prodA|regE|29147.939313541443|\n",
      "|prodB|regF| 15182.29534355441|\n",
      "|prodD|regE| 50465.05578580952|\n",
      "|prodK|regB| 31934.40395342406|\n",
      "|prodF|regD|30099.764020475784|\n",
      "|prodH|regE| 9780.791256960383|\n",
      "|prodB|regA|15173.427162975999|\n",
      "|prodD|regF|14644.907916090564|\n",
      "|prodG|regC|34617.725516208724|\n",
      "|prodB|regE|30647.927342034312|\n",
      "|prodK|regF|25307.428539388253|\n",
      "|prodD|regD|32346.278487783064|\n",
      "|prodM|regC| 7226.084397656817|\n",
      "|prodL|regA|32890.519567608215|\n",
      "|prodM|regD| 18403.31832397603|\n",
      "|prodG|regF|  33858.2808189527|\n",
      "|prodF|regE|1084.5768830938393|\n",
      "|prodE|regF| 46132.38800533221|\n",
      "|prodI|regC| 31181.94863954508|\n",
      "+-----+----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy([\"prod\",\"geo\"]).sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------------------+\n",
      "| prod| geo|          sum(val)|\n",
      "+-----+----+------------------+\n",
      "|prodI|regB| 26614.21261323439|\n",
      "|prodA|regE|29147.939313541443|\n",
      "|prodB|regF| 15182.29534355441|\n",
      "|prodD|regE| 50465.05578580952|\n",
      "|prodK|regB| 31934.40395342406|\n",
      "|prodF|regD|30099.764020475784|\n",
      "|prodH|regE| 9780.791256960383|\n",
      "|prodB|regA|15173.427162975999|\n",
      "|prodD|regF|14644.907916090564|\n",
      "|prodG|regC|34617.725516208724|\n",
      "|prodB|regE|30647.927342034312|\n",
      "|prodK|regF|25307.428539388253|\n",
      "|prodD|regD|32346.278487783064|\n",
      "|prodM|regC| 7226.084397656817|\n",
      "|prodL|regA|32890.519567608215|\n",
      "|prodM|regD| 18403.31832397603|\n",
      "|prodG|regF|  33858.2808189527|\n",
      "|prodF|regE|1084.5768830938393|\n",
      "|prodE|regF| 46132.38800533221|\n",
      "|prodI|regC| 31181.94863954508|\n",
      "+-----+----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select prod, geo, sum(val) from train_df group by prod, geo\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Where (filter)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------------------+\n",
      "| geo| prod|               val|\n",
      "+----+-----+------------------+\n",
      "|regF|prodB| 3941.612243931777|\n",
      "|regA|prodD| 7851.067371046244|\n",
      "|regA|prodF| 4412.135940077227|\n",
      "|regF|prodE| 7768.088411993063|\n",
      "|regC|prodC| 8487.318566397455|\n",
      "|regB|prodH|3270.4437860414255|\n",
      "|regB|prodI| 1737.156094854783|\n",
      "|regB|prodL|1174.4495834500963|\n",
      "|regF|prodE| 5854.771431302804|\n",
      "|regA|prodJ| 9999.170456582882|\n",
      "|regF|prodM| 4149.643093481655|\n",
      "|regF|prodK| 6851.296839248875|\n",
      "|regE|prodI|8031.0974464980545|\n",
      "|regE|prodK| 7788.098836535736|\n",
      "|regD|prodI|   9546.5055944439|\n",
      "|regA|prodE|2794.2461412627963|\n",
      "|regA|prodK|1183.4201558829911|\n",
      "|regA|prodB|2371.4411120631135|\n",
      "|regB|prodC|2135.9887738678563|\n",
      "|regB|prodE| 7025.129601579196|\n",
      "+----+-----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(df[\"prod\"] != \"prodA\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from train_df where prod != 'prodA'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------------------+\n",
      "| geo| prod|               val|\n",
      "+----+-----+------------------+\n",
      "|regA|prodA| 6865.791897195005|\n",
      "|regC|prodA|7215.5297433968235|\n",
      "|regA|prodA| 6403.704533351597|\n",
      "|regA|prodA| 2089.495585605836|\n",
      "|regA|prodA| 8458.365799022124|\n",
      "|regE|prodA| 8273.792969260803|\n",
      "|regC|prodA|  8476.57917001543|\n",
      "|regD|prodA| 5182.376199429085|\n",
      "|regB|prodA| 5417.392015707275|\n",
      "|regA|prodA|2075.9926712954757|\n",
      "|regD|prodA| 1904.734972817117|\n",
      "|regB|prodA| 4964.219183224092|\n",
      "|regB|prodA| 4606.366980267458|\n",
      "|regF|prodA|  8741.49999243562|\n",
      "|regE|prodA| 6589.245889649337|\n",
      "|regD|prodA| 4888.257977446953|\n",
      "|regA|prodA| 6918.943478348161|\n",
      "|regA|prodA| 5883.342349214755|\n",
      "|regD|prodA| 9930.314254556977|\n",
      "|regD|prodA| 5961.385691924227|\n",
      "+----+-----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# like\n",
    "df.where(df.prod.like('%A')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from train_df where prod like '%A'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Order by**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------------------+\n",
      "| prod| geo|          sum(val)|\n",
      "+-----+----+------------------+\n",
      "|prodF|regA|16159.108540624577|\n",
      "|prodA|regA| 60940.39816691554|\n",
      "|prodI|regA| 4920.170437035049|\n",
      "|prodK|regA|19950.005918466282|\n",
      "|prodE|regA| 63855.75514005185|\n",
      "|prodB|regA|15173.427162975999|\n",
      "|prodJ|regA| 31533.65279126246|\n",
      "|prodD|regA| 36200.10333060895|\n",
      "|prodH|regA| 33347.42827483338|\n",
      "|prodC|regA|27816.734886044535|\n",
      "|prodL|regA|32890.519567608215|\n",
      "|prodM|regA| 58332.29058567657|\n",
      "|prodG|regA| 43771.03604185874|\n",
      "|prodK|regB| 31934.40395342406|\n",
      "|prodJ|regB|22852.827988291472|\n",
      "|prodC|regB| 38666.67273025063|\n",
      "|prodL|regB|21662.464276897255|\n",
      "|prodM|regB| 61621.07292795643|\n",
      "|prodG|regB|28958.581434979576|\n",
      "|prodE|regB|27459.596285651718|\n",
      "+-----+----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy([\"prod\",\"geo\"]).sum().orderBy(\"geo\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select prod, geo, sum(val) from train_df group by prod, geo order by geo\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wiele agregacji + aliasy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------------------+------------------+\n",
      "| prod| geo|           val_sum|           val_avg|\n",
      "+-----+----+------------------+------------------+\n",
      "|prodI|regB| 26614.21261323439|3802.0303733191986|\n",
      "|prodA|regE|29147.939313541443| 5829.587862708288|\n",
      "|prodB|regF| 15182.29534355441|3795.5738358886024|\n",
      "|prodD|regE| 50465.05578580952| 7209.293683687074|\n",
      "|prodK|regB| 31934.40395342406|  5322.40065890401|\n",
      "|prodF|regD|30099.764020475784| 6019.952804095157|\n",
      "|prodH|regE| 9780.791256960383| 3260.263752320128|\n",
      "|prodB|regA|15173.427162975999| 5057.809054325333|\n",
      "|prodD|regF|14644.907916090564| 2928.981583218113|\n",
      "|prodG|regC|34617.725516208724|4327.2156895260905|\n",
      "|prodB|regE|30647.927342034312| 5107.987890339052|\n",
      "|prodK|regF|25307.428539388253|  5061.48570787765|\n",
      "|prodD|regD|32346.278487783064|  5391.04641463051|\n",
      "|prodM|regC| 7226.084397656817| 7226.084397656817|\n",
      "|prodL|regA|32890.519567608215| 6578.103913521643|\n",
      "|prodM|regD| 18403.31832397603| 6134.439441325343|\n",
      "|prodG|regF|  33858.2808189527|3762.0312021058553|\n",
      "|prodF|regE|1084.5768830938393|1084.5768830938393|\n",
      "|prodE|regF| 46132.38800533221| 6590.341143618886|\n",
      "|prodI|regC| 31181.94863954508| 6236.389727909016|\n",
      "+-----+----+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy([\"prod\",\"geo\"]).agg(f.sum(\"val\").alias(\"val_sum\"), f.avg(\"val\").alias(\"val_avg\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select prod, geo, sum(val) as val_sum, avg(val) as val_avg from train_df group by prod, geo\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Joiny**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------------------+------+--------+\n",
      "| geo| prod|               val|geo_id|geo_name|\n",
      "+----+-----+------------------+------+--------+\n",
      "|regB|prodH|3270.4437860414255|  regB|Region B|\n",
      "|regB|prodI| 1737.156094854783|  regB|Region B|\n",
      "|regB|prodL|1174.4495834500963|  regB|Region B|\n",
      "|regB|prodC|2135.9887738678563|  regB|Region B|\n",
      "|regB|prodE| 7025.129601579196|  regB|Region B|\n",
      "|regB|prodC|2510.1022903102203|  regB|Region B|\n",
      "|regB|prodF|1972.8887469184592|  regB|Region B|\n",
      "|regB|prodI| 4569.889973649646|  regB|Region B|\n",
      "|regB|prodK|1917.1117490137888|  regB|Region B|\n",
      "|regB|prodG| 7952.327964561016|  regB|Region B|\n",
      "|regB|prodF| 5843.556158034302|  regB|Region B|\n",
      "|regB|prodC| 8665.418440173988|  regB|Region B|\n",
      "|regB|prodF| 8934.680656024948|  regB|Region B|\n",
      "|regB|prodK| 2636.558714419226|  regB|Region B|\n",
      "|regB|prodM| 4063.049588478553|  regB|Region B|\n",
      "|regB|prodM| 7823.481881342198|  regB|Region B|\n",
      "|regB|prodA| 5417.392015707275|  regB|Region B|\n",
      "|regB|prodC| 8136.395813190857|  regB|Region B|\n",
      "|regB|prodL|  5778.29738031191|  regB|Region B|\n",
      "|regB|prodC| 7325.694479671921|  regB|Region B|\n",
      "+----+-----+------------------+------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dostępne: inner, cross, outer, full, full_outer, left, left_outer, right, right_outer, left_semi, i left_anti\n",
    "df.join(geo_df, df.geo == geo_df.geo_id, \"inner\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "332"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.join(geo_df, df.geo == geo_df.geo_id, \"inner\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"\"\"select train_df.prod, train_df.geo, train_df.val, geo_df.geo_name \\\n",
    "from train_df inner join geo_df on train_df.geo == geo_df.geo_id\"\"\"\n",
    "spark.sql(q).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------------------+------+--------+\n",
      "| geo| prod|               val|geo_id|geo_name|\n",
      "+----+-----+------------------+------+--------+\n",
      "|regB|prodH|3270.4437860414255|  regB|Region B|\n",
      "|regB|prodI| 1737.156094854783|  regB|Region B|\n",
      "|regB|prodL|1174.4495834500963|  regB|Region B|\n",
      "|regB|prodC|2135.9887738678563|  regB|Region B|\n",
      "|regB|prodE| 7025.129601579196|  regB|Region B|\n",
      "|regB|prodC|2510.1022903102203|  regB|Region B|\n",
      "|regB|prodF|1972.8887469184592|  regB|Region B|\n",
      "|regB|prodI| 4569.889973649646|  regB|Region B|\n",
      "|regB|prodK|1917.1117490137888|  regB|Region B|\n",
      "|regB|prodG| 7952.327964561016|  regB|Region B|\n",
      "|regB|prodF| 5843.556158034302|  regB|Region B|\n",
      "|regB|prodC| 8665.418440173988|  regB|Region B|\n",
      "|regB|prodF| 8934.680656024948|  regB|Region B|\n",
      "|regB|prodK| 2636.558714419226|  regB|Region B|\n",
      "|regB|prodM| 4063.049588478553|  regB|Region B|\n",
      "|regB|prodM| 7823.481881342198|  regB|Region B|\n",
      "|regB|prodA| 5417.392015707275|  regB|Region B|\n",
      "|regB|prodC| 8136.395813190857|  regB|Region B|\n",
      "|regB|prodL|  5778.29738031191|  regB|Region B|\n",
      "|regB|prodC| 7325.694479671921|  regB|Region B|\n",
      "+----+-----+------------------+------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.join(geo_df, df.geo == geo_df.geo_id, \"outer\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.join(geo_df, df.geo == geo_df.geo_id, \"outer\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"\"\"select train_df.prod, train_df.geo, train_df.val, geo_df.geo_name \\\n",
    "from train_df full outer join geo_df on train_df.geo == geo_df.geo_id\"\"\"\n",
    "spark.sql(q).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"\"\"select train_df.prod, train_df.geo, train_df.val, geo_df.geo_name \\\n",
    "from train_df full outer join geo_df on train_df.geo == geo_df.geo_id\"\"\"\n",
    "spark.sql(q).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distinct**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+\n",
      "| prod| geo|\n",
      "+-----+----+\n",
      "|prodI|regB|\n",
      "|prodA|regE|\n",
      "|prodB|regF|\n",
      "|prodD|regE|\n",
      "|prodK|regB|\n",
      "|prodF|regD|\n",
      "|prodH|regE|\n",
      "|prodB|regA|\n",
      "|prodD|regF|\n",
      "|prodG|regC|\n",
      "|prodB|regE|\n",
      "|prodK|regF|\n",
      "|prodD|regD|\n",
      "|prodM|regC|\n",
      "|prodL|regA|\n",
      "|prodM|regD|\n",
      "|prodG|regF|\n",
      "|prodF|regE|\n",
      "|prodE|regF|\n",
      "|prodI|regC|\n",
      "+-----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select([\"prod\", \"geo\"]).distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select distinct prod, geo from train_df\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+\n",
      "| prod| geo|\n",
      "+-----+----+\n",
      "|prodI|regB|\n",
      "|prodA|regE|\n",
      "|prodB|regF|\n",
      "|prodD|regE|\n",
      "|prodK|regB|\n",
      "|prodF|regD|\n",
      "|prodH|regE|\n",
      "|prodB|regA|\n",
      "|prodD|regF|\n",
      "|prodG|regC|\n",
      "|prodB|regE|\n",
      "|prodK|regF|\n",
      "|prodD|regD|\n",
      "|prodM|regC|\n",
      "|prodL|regA|\n",
      "|prodM|regD|\n",
      "|prodG|regF|\n",
      "|prodF|regE|\n",
      "|prodE|regF|\n",
      "|prodI|regC|\n",
      "+-----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# alternatywnie\n",
    "df.select([\"prod\", \"geo\"]).dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Usuwanie NULLi**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------------------+------+--------+\n",
      "| geo| prod|               val|geo_id|geo_name|\n",
      "+----+-----+------------------+------+--------+\n",
      "|regB|prodH|3270.4437860414255|  regB|Region B|\n",
      "|regB|prodI| 1737.156094854783|  regB|Region B|\n",
      "|regB|prodL|1174.4495834500963|  regB|Region B|\n",
      "|regB|prodC|2135.9887738678563|  regB|Region B|\n",
      "|regB|prodE| 7025.129601579196|  regB|Region B|\n",
      "|regB|prodC|2510.1022903102203|  regB|Region B|\n",
      "|regB|prodF|1972.8887469184592|  regB|Region B|\n",
      "|regB|prodI| 4569.889973649646|  regB|Region B|\n",
      "|regB|prodK|1917.1117490137888|  regB|Region B|\n",
      "|regB|prodG| 7952.327964561016|  regB|Region B|\n",
      "|regB|prodF| 5843.556158034302|  regB|Region B|\n",
      "|regB|prodC| 8665.418440173988|  regB|Region B|\n",
      "|regB|prodF| 8934.680656024948|  regB|Region B|\n",
      "|regB|prodK| 2636.558714419226|  regB|Region B|\n",
      "|regB|prodM| 4063.049588478553|  regB|Region B|\n",
      "|regB|prodM| 7823.481881342198|  regB|Region B|\n",
      "|regB|prodA| 5417.392015707275|  regB|Region B|\n",
      "|regB|prodC| 8136.395813190857|  regB|Region B|\n",
      "|regB|prodL|  5778.29738031191|  regB|Region B|\n",
      "|regB|prodC| 7325.694479671921|  regB|Region B|\n",
      "+----+-----+------------------+------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# \"any\" or \"all\"\n",
    "df.join(geo_df, df.geo == geo_df.geo_id, \"outer\").dropna(\"any\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "332"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.join(geo_df, df.geo == geo_df.geo_id, \"outer\").dropna(\"any\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"\"\"select * \\\n",
    "from train_df full outer join geo_df on train_df.geo == geo_df.geo_id \\\n",
    "where geo_name is not null\"\"\"\n",
    "spark.sql(q).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"\"\"select * \\\n",
    "from train_df full outer join geo_df on train_df.geo == geo_df.geo_id \\\n",
    "where geo_name is not null\"\"\"\n",
    "spark.sql(q).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zastępowanie NULLi**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------------------+-----------+-----------+\n",
      "| geo| prod|               val|     geo_id|   geo_name|\n",
      "+----+-----+------------------+-----------+-----------+\n",
      "|regF|prodB| 3941.612243931777|replacement|replacement|\n",
      "|regF|prodE| 7768.088411993063|replacement|replacement|\n",
      "|regF|prodE| 5854.771431302804|replacement|replacement|\n",
      "|regF|prodM| 4149.643093481655|replacement|replacement|\n",
      "|regF|prodK| 6851.296839248875|replacement|replacement|\n",
      "|regF|prodE| 6465.227445018385|replacement|replacement|\n",
      "|regF|prodE| 7731.330867597572|replacement|replacement|\n",
      "|regF|prodJ| 2030.795842410079|replacement|replacement|\n",
      "|regF|prodH| 7435.132345445216|replacement|replacement|\n",
      "|regF|prodH| 8646.180642996196|replacement|replacement|\n",
      "|regF|prodL| 7604.157736120527|replacement|replacement|\n",
      "|regF|prodJ| 5351.477534948765|replacement|replacement|\n",
      "|regF|prodG| 3415.559842443028|replacement|replacement|\n",
      "|regF|prodK|1258.2999799308063|replacement|replacement|\n",
      "|regF|prodI| 8922.879817824029|replacement|replacement|\n",
      "|regF|prodJ|3486.4171818056643|replacement|replacement|\n",
      "|regF|prodL| 1091.360029948094|replacement|replacement|\n",
      "|regF|prodC| 9532.563199993281|replacement|replacement|\n",
      "|regF|prodM| 5397.200621639086|replacement|replacement|\n",
      "|regF|prodG| 6813.126097668571|replacement|replacement|\n",
      "+----+-----+------------------+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.join(geo_df, df.geo == geo_df.geo_id, \"outer\").fillna(\"replacement\").where(\"geo_name == 'replacement'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"\"\"select prod, geo, val, if(geo_name is null, 'replacement', geo_name) as geo_name \\\n",
    "from train_df full outer join geo_df on train_df.geo == geo_df.geo_id \\\n",
    "order by geo_name desc\"\"\"\n",
    "spark.sql(q).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Podmiana wartości**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+------------------+\n",
      "| geo|     prod|               val|\n",
      "+----+---------+------------------+\n",
      "|regF|    prodB| 3941.612243931777|\n",
      "|regA|    prodD| 7851.067371046244|\n",
      "|regA|    prodF| 4412.135940077227|\n",
      "|regF|    prodE| 7768.088411993063|\n",
      "|regC|    prodC| 8487.318566397455|\n",
      "|regB|    prodH|3270.4437860414255|\n",
      "|regB|    prodI| 1737.156094854783|\n",
      "|regB|    prodL|1174.4495834500963|\n",
      "|regF|    prodE| 5854.771431302804|\n",
      "|regA|    prodJ| 9999.170456582882|\n",
      "|regF|    prodM| 4149.643093481655|\n",
      "|regF|    prodK| 6851.296839248875|\n",
      "|regE|    prodI|8031.0974464980545|\n",
      "|regA|Product A| 6865.791897195005|\n",
      "|regE|    prodK| 7788.098836535736|\n",
      "|regD|    prodI|   9546.5055944439|\n",
      "|regA|    prodE|2794.2461412627963|\n",
      "|regA|    prodK|1183.4201558829911|\n",
      "|regA|    prodB|2371.4411120631135|\n",
      "|regB|    prodC|2135.9887738678563|\n",
      "+----+---------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.replace(\"prodA\", \"Product A\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------------------+\n",
      "| geo| prod|               val|\n",
      "+----+-----+------------------+\n",
      "|regF|prodB| 3941.612243931777|\n",
      "|regA|prodD| 7851.067371046244|\n",
      "|regA|prodF| 4412.135940077227|\n",
      "|regF|prodE| 7768.088411993063|\n",
      "|regC|prodC| 8487.318566397455|\n",
      "|regB|prodH|3270.4437860414255|\n",
      "|regB|prodI| 1737.156094854783|\n",
      "|regB|prodL|1174.4495834500963|\n",
      "|regF|prodE| 5854.771431302804|\n",
      "|regA|prodJ| 9999.170456582882|\n",
      "|regF|prodM| 4149.643093481655|\n",
      "|regF|prodK| 6851.296839248875|\n",
      "|regE|prodI|8031.0974464980545|\n",
      "|regA|prodA| 6865.791897195005|\n",
      "|regE|prodK| 7788.098836535736|\n",
      "|regD|prodI|   9546.5055944439|\n",
      "|regA|prodE|2794.2461412627963|\n",
      "|regA|prodK|1183.4201558829911|\n",
      "|regA|prodB|2371.4411120631135|\n",
      "|regB|prodC|2135.9887738678563|\n",
      "+----+-----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select geo, regexp_replace(prod, 'prodA', 'Product A') as prod, val from train_df\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zmiana nazw kolumn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------------------+\n",
      "| geo| prod|            volume|\n",
      "+----+-----+------------------+\n",
      "|regF|prodB| 3941.612243931777|\n",
      "|regA|prodD| 7851.067371046244|\n",
      "|regA|prodF| 4412.135940077227|\n",
      "|regF|prodE| 7768.088411993063|\n",
      "|regC|prodC| 8487.318566397455|\n",
      "|regB|prodH|3270.4437860414255|\n",
      "|regB|prodI| 1737.156094854783|\n",
      "|regB|prodL|1174.4495834500963|\n",
      "|regF|prodE| 5854.771431302804|\n",
      "|regA|prodJ| 9999.170456582882|\n",
      "|regF|prodM| 4149.643093481655|\n",
      "|regF|prodK| 6851.296839248875|\n",
      "|regE|prodI|8031.0974464980545|\n",
      "|regA|prodA| 6865.791897195005|\n",
      "|regE|prodK| 7788.098836535736|\n",
      "|regD|prodI|   9546.5055944439|\n",
      "|regA|prodE|2794.2461412627963|\n",
      "|regA|prodK|1183.4201558829911|\n",
      "|regA|prodB|2371.4411120631135|\n",
      "|regB|prodC|2135.9887738678563|\n",
      "+----+-----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumnRenamed(\"val\", \"volume\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select prod, geo, val as volume from train_df\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tworzenie nowej kolumny**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------------------+------------------+\n",
      "| geo| prod|               val|              val2|\n",
      "+----+-----+------------------+------------------+\n",
      "|regF|prodB| 3941.612243931777| 39.41612243931777|\n",
      "|regA|prodD| 7851.067371046244| 78.51067371046244|\n",
      "|regA|prodF| 4412.135940077227| 44.12135940077227|\n",
      "|regF|prodE| 7768.088411993063| 77.68088411993064|\n",
      "|regC|prodC| 8487.318566397455| 84.87318566397454|\n",
      "|regB|prodH|3270.4437860414255| 32.70443786041425|\n",
      "|regB|prodI| 1737.156094854783| 17.37156094854783|\n",
      "|regB|prodL|1174.4495834500963|11.744495834500963|\n",
      "|regF|prodE| 5854.771431302804| 58.54771431302804|\n",
      "|regA|prodJ| 9999.170456582882| 99.99170456582883|\n",
      "|regF|prodM| 4149.643093481655| 41.49643093481655|\n",
      "|regF|prodK| 6851.296839248875| 68.51296839248874|\n",
      "|regE|prodI|8031.0974464980545| 80.31097446498055|\n",
      "|regA|prodA| 6865.791897195005| 68.65791897195005|\n",
      "|regE|prodK| 7788.098836535736| 77.88098836535737|\n",
      "|regD|prodI|   9546.5055944439| 95.46505594443899|\n",
      "|regA|prodE|2794.2461412627963|27.942461412627964|\n",
      "|regA|prodK|1183.4201558829911| 11.83420155882991|\n",
      "|regA|prodB|2371.4411120631135|23.714411120631134|\n",
      "|regB|prodC|2135.9887738678563|21.359887738678562|\n",
      "+----+-----+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"val2\", df[\"val\"] / 100).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select prod, geo, val, val/100 as val2 from train_df\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| prod|out|\n",
      "+-----+---+\n",
      "|prodB|  2|\n",
      "|prodD|  1|\n",
      "|prodF|  2|\n",
      "|prodE|  1|\n",
      "|prodC|  1|\n",
      "|prodH|  2|\n",
      "|prodI|  3|\n",
      "|prodL|  3|\n",
      "|prodE|  2|\n",
      "|prodJ|  1|\n",
      "|prodM|  2|\n",
      "|prodK|  2|\n",
      "|prodI|  1|\n",
      "|prodA|  2|\n",
      "|prodK|  1|\n",
      "|prodI|  1|\n",
      "|prodE|  2|\n",
      "|prodK|  3|\n",
      "|prodB|  3|\n",
      "|prodC|  3|\n",
      "+-----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.prod, f.when(df.val > 7500, 1).when(df.val < 2500, 3).otherwise(2).alias(\"out\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"\"\"select prod, case when val > 7500 then 1 \\\n",
    "when val <= 7500 and val >= 2500 then 2 \\\n",
    "when val < 2500 then 3 end as out from train_df\"\"\"\n",
    "spark.sql(q).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Substring**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "| out|\n",
      "+----+\n",
      "|rodB|\n",
      "|rodD|\n",
      "|rodF|\n",
      "|rodE|\n",
      "|rodC|\n",
      "|rodH|\n",
      "|rodI|\n",
      "|rodL|\n",
      "|rodE|\n",
      "|rodJ|\n",
      "|rodM|\n",
      "|rodK|\n",
      "|rodI|\n",
      "|rodA|\n",
      "|rodK|\n",
      "|rodI|\n",
      "|rodE|\n",
      "|rodK|\n",
      "|rodB|\n",
      "|rodC|\n",
      "+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.prod.substr(2, 4).alias(\"out\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-290-4c1ae156bd9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'prod'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/spark/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m             raise AttributeError(\n\u001b[0;32m-> 1182\u001b[0;31m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[1;32m   1183\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "df.select(df.prod).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select substring(prod, 2, 4) as val2 from train_df\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Funkcje analityczne (window functions)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+------------------+\n",
      "| prod|               val|          prod_val|\n",
      "+-----+------------------+------------------+\n",
      "|prodE| 7768.088411993063|200344.20973732683|\n",
      "|prodE| 5854.771431302804|200344.20973732683|\n",
      "|prodE|2794.2461412627963|200344.20973732683|\n",
      "|prodE| 7025.129601579196|200344.20973732683|\n",
      "|prodE| 6465.227445018385|200344.20973732683|\n",
      "|prodE| 7731.330867597572|200344.20973732683|\n",
      "|prodE| 9623.555543853063|200344.20973732683|\n",
      "|prodE|1683.9683005875395|200344.20973732683|\n",
      "|prodE| 8652.639151799167|200344.20973732683|\n",
      "|prodE| 4335.137885962506|200344.20973732683|\n",
      "|prodE|1779.9660825010696|200344.20973732683|\n",
      "|prodE| 3977.121674763371|200344.20973732683|\n",
      "|prodE| 7521.265009837464|200344.20973732683|\n",
      "|prodE|  5807.21580340897|200344.20973732683|\n",
      "|prodE| 5980.230701187475|200344.20973732683|\n",
      "|prodE|1462.3904546522006|200344.20973732683|\n",
      "|prodE| 8399.040807694873|200344.20973732683|\n",
      "|prodE| 4984.489036173954|200344.20973732683|\n",
      "|prodE| 8090.452239820582|200344.20973732683|\n",
      "|prodE|  6950.64738568031|200344.20973732683|\n",
      "+-----+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowSpec = Window.partitionBy('prod')# jakgroupby tylko zwraca wartości niepogrupowane\n",
    "\n",
    "df.select(\"prod\", \"val\", f.sum(\"val\").over(windowSpec).alias(\"prod_val\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select prod, val, sum(val) over (partition by prod) as prod_val from train_df\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Ćwiczenia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importy i przygotowanie danych**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.window import Window\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('my_app').master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "goBike = spark.read.csv(\"./2017-fordgobike-tripdata.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- duration_sec: integer (nullable = true)\n",
      " |-- start_time: timestamp (nullable = true)\n",
      " |-- end_time: timestamp (nullable = true)\n",
      " |-- start_station_id: integer (nullable = true)\n",
      " |-- start_station_name: string (nullable = true)\n",
      " |-- start_station_latitude: double (nullable = true)\n",
      " |-- start_station_longitude: double (nullable = true)\n",
      " |-- end_station_id: integer (nullable = true)\n",
      " |-- end_station_name: string (nullable = true)\n",
      " |-- end_station_latitude: double (nullable = true)\n",
      " |-- end_station_longitude: double (nullable = true)\n",
      " |-- bike_id: integer (nullable = true)\n",
      " |-- user_type: string (nullable = true)\n",
      " |-- member_birth_year: integer (nullable = true)\n",
      " |-- member_gender: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "goBike.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "519700"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goBike.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(goBike.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **ZADANIE 1**: Zapoznaj się z danymi:\n",
    "1. Pozbądź się wierszy zawierających NULLe\n",
    "1. Sprawdź rozkład zmiennej \"member_gender\"\n",
    "2. Oblicz minimalny, maksymalny i średni wiek osób wypożyczających rowery\n",
    "3. Oblicz liczbę unikalnych rowerów\n",
    "4. Oblicz liczbę unikalnych stacji\n",
    "5. Sprawdź który rower był wypożyczony najdłużej a który najkrócej w ciągu analizowanego okresu (oraz jak długo)\n",
    "6. Oblicz średni czas pojedynczego wypożyczenia\n",
    "7. Sprawdź pomiędzy którymi stacjami występował największy ruch (hint: A -> B == B -> A)\n",
    "8. Sprawdź o której godzinie w ciągu dnia wypożyczano najwięcej rowerów\n",
    "9. Sprawdź *średnią liczbę wypożyczeń* dla poszczególnych dni tygodnia (hint: java.text.SimpleDateFormat)\n",
    "10. **⋆** Oblicz średni dystans (w km) pomiędzy stacją początkową a końcową dla wszystkich wypożyczeń"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "goBike = goBike.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "453159"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goBike.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+\n",
      "|member_gender| count|\n",
      "+-------------+------+\n",
      "|       Female| 98542|\n",
      "|        Other|  6299|\n",
      "|         Male|348318|\n",
      "+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "goBike.groupBy('member_gender').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|member_gender|\n",
      "+-------------+\n",
      "|       Female|\n",
      "|        Other|\n",
      "|         Male|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "goBike.select('member_gender').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "goBike = goBike.withColumn('age',2018 - goBike[\"member_birth_year\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['duration_sec',\n",
       " 'start_time',\n",
       " 'end_time',\n",
       " 'start_station_id',\n",
       " 'start_station_name',\n",
       " 'start_station_latitude',\n",
       " 'start_station_longitude',\n",
       " 'end_station_id',\n",
       " 'end_station_name',\n",
       " 'end_station_latitude',\n",
       " 'end_station_longitude',\n",
       " 'bike_id',\n",
       " 'user_type',\n",
       " 'member_birth_year',\n",
       " 'member_gender',\n",
       " 'age']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goBike.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|               age|\n",
      "+-------+------------------+\n",
      "|  count|            453159|\n",
      "|   mean|37.595212717831934|\n",
      "| stddev|10.513487539908406|\n",
      "|    min|                19|\n",
      "|    max|               132|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "goBike.select('age').describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------------------+\n",
      "|min_age|max_age|           avg_age|\n",
      "+-------+-------+------------------+\n",
      "|     19|    132|37.595212717831934|\n",
      "+-------+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "goBike.select((2018 - goBike[\"member_birth_year\"]).alias('age'))\\\n",
    ".agg(f.min('age').alias('min_age'),f.max('age').alias('max_age'),f.avg('age').alias('avg_age')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3670"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goBike.select('bike_id').distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "272"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goBike.select('start_station_id')\\\n",
    ".union(goBike.select('end_station_id')).dropDuplicates().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[max(duration_sec): int]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goBike.agg(f.max('duration_sec'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(bike_id=2178, sum_sec=377954)]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goBike.groupBy('bike_id').agg(f.sum('duration_sec').alias('sum_sec'))\\\n",
    ".orderBy('sum_sec',ascending=False).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(bike_id=2609, sum_sec=74)]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goBike.groupBy('bike_id').agg(f.sum('duration_sec').alias('sum_sec'))\\\n",
    ".orderBy('sum_sec',ascending=True).take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = goBike.groupBy('bike_id').agg(f.sum('duration_sec').alias('sum_sec'))\\\n",
    ".orderBy('sum_sec',ascending=True).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(bike_id=2609, sum_sec=74)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(bike_id=2178, sum_sec=377954)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "377954"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values[-1].sum_sec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+\n",
      "|round(avg(duration_sec), 1)|\n",
      "+---------------------------+\n",
      "|                      832.9|\n",
      "+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "goBike.agg(f.round(f.avg('duration_sec'),1)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------+-----+\n",
      "|start_station_id|end_station_id|count|\n",
      "+----------------+--------------+-----+\n",
      "|             259|           259|   16|\n",
      "|             193|           196|   74|\n",
      "|              15|            14|   70|\n",
      "|              28|            16|  224|\n",
      "|             181|           198|   53|\n",
      "|             237|           227|    7|\n",
      "|             108|           112|   92|\n",
      "|             163|             7|   58|\n",
      "|              15|            26|   46|\n",
      "|             133|           323|    2|\n",
      "|              61|            13|   10|\n",
      "|             195|           179|   27|\n",
      "|             114|           223|   43|\n",
      "|               3|            22|  158|\n",
      "|              75|            97|   34|\n",
      "|              58|            81|   93|\n",
      "|             295|           295|   11|\n",
      "|              63|            81|   20|\n",
      "|             127|           109|   75|\n",
      "|              43|           100|   13|\n",
      "+----------------+--------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "goBike.groupBy('start_station_id','end_station_id').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_end = goBike.where(goBike['start_station_id'] <= goBike['end_station_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_start= goBike.where(goBike['start_station_id'] > goBike['end_station_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[end_station_id: int, start_station_id: int]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_start.select('end_station_id','start_station_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(start_station_id=6, end_station_id=15, count=3282)]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_end.select('start_station_id','end_station_id').union(end_start.select('end_station_id','start_station_id'))\\\n",
    ".groupBy('start_station_id','end_station_id').count().orderBy('count',ascending=False).take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_hour = goBike.select(f.hour('start_time').alias('hour')).groupBy('hour').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hour', 'count']"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_hour.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(hour=8, count=54678)]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_hour.orderBy('count',ascending=False).take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_week = goBike.select(f.dayofweek('start_time').alias('day'),f.weekofyear('start_time').alias('week'))\\\n",
    ".groupBy('day','week').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+\n",
      "|day|week|count|\n",
      "+---+----+-----+\n",
      "|  5|  49| 4066|\n",
      "|  5|  40| 3847|\n",
      "|  3|  30| 1845|\n",
      "|  6|  37| 3273|\n",
      "|  7|  33| 1289|\n",
      "|  5|  33| 2831|\n",
      "|  1|  37| 1795|\n",
      "|  2|  45| 3860|\n",
      "|  2|  40| 3752|\n",
      "|  4|  47| 2588|\n",
      "|  7|  37| 1811|\n",
      "|  1|  46| 1477|\n",
      "|  7|  31| 1050|\n",
      "|  3|  43| 4070|\n",
      "|  6|  42| 3139|\n",
      "|  3|  41| 3653|\n",
      "|  2|  49| 3821|\n",
      "|  1|  32| 1177|\n",
      "|  1|  49| 1614|\n",
      "|  4|  27|  974|\n",
      "+---+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "day_week.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "|day|        avg(count)|\n",
      "+---+------------------+\n",
      "|  1|1214.3703703703704|\n",
      "|  6|2667.3333333333335|\n",
      "|  3|3095.4615384615386|\n",
      "|  5| 2859.185185185185|\n",
      "|  4| 2976.185185185185|\n",
      "|  7|1362.2222222222222|\n",
      "|  2|2828.3076923076924|\n",
      "+---+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "day_week.groupBy('day').agg(f.avg('count')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['duration_sec',\n",
       " 'start_time',\n",
       " 'end_time',\n",
       " 'start_station_id',\n",
       " 'start_station_name',\n",
       " 'start_station_latitude',\n",
       " 'start_station_longitude',\n",
       " 'end_station_id',\n",
       " 'end_station_name',\n",
       " 'end_station_latitude',\n",
       " 'end_station_longitude',\n",
       " 'bike_id',\n",
       " 'user_type',\n",
       " 'member_birth_year',\n",
       " 'member_gender',\n",
       " 'age']"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goBike.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|summary|end_station_latitude|\n",
      "+-------+--------------------+\n",
      "|  count|              453159|\n",
      "|   mean|   37.77209035033914|\n",
      "| stddev| 0.08481063344027019|\n",
      "|    min|          37.3172979|\n",
      "|    max|   37.88022244590679|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "goBike.select('end_station_latitude').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goBike.withColumn('distance', 2 * 6378.14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn(\"val2\", df[\"val\"] / 100).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "defdef  haversinehaversin (pt1,pt2):\n",
    "    '''\n",
    "    The haversine formula determines the great-circle distance \n",
    "    between two points on a sphere given their longitudes and latitudes. \n",
    "    '''\n",
    "    r = 6378.14\n",
    "    (lat1,lon1) = np.deg2rad(pt1)\n",
    "    (lat2,lon2) = np.deg2rad(pt2)\n",
    "    \n",
    "    sin_lat = np.sin((lat2-lat1)/2)\n",
    "    sin_lon = np.sin((lon2-lon1)/2)\n",
    "    \n",
    "    return 2 * r * np.arcsin(np.sqrt(sin_lat * sin_lat + np.cos(lat1) * np.cos(lat2) * sin_lon * sin_lon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **ZADANIE 2**: Utwórz DataFrame `dataDaily` zawierający dane zagregowane do poziomu dnia. Zbiór ma zawierać następujące informacje (kolumny): \n",
    "- 'date' : data \n",
    "- 'avg_duration_sec' : średni czas wypożyczeń danego dnia\n",
    "- 'n_trips' : liczba wypożyczeń danego dnia\n",
    "- 'n_bikes' : liczba unikatowych rowerów użytych danego dnia\n",
    "- 'n_routes' : liczba unikatowych kombinacji stacji (x -> y == y -> x) danego dnia\n",
    "- 'n_subscriber' : liczba wypożyczeń dokonanych przez subskrybentów danego dnia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "goBike = goBike.withColumn('date',f.date_format('start_time','dd.MM.yy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['duration_sec',\n",
       " 'start_time',\n",
       " 'end_time',\n",
       " 'start_station_id',\n",
       " 'start_station_name',\n",
       " 'start_station_latitude',\n",
       " 'start_station_longitude',\n",
       " 'end_station_id',\n",
       " 'end_station_name',\n",
       " 'end_station_latitude',\n",
       " 'end_station_longitude',\n",
       " 'bike_id',\n",
       " 'user_type',\n",
       " 'member_birth_year',\n",
       " 'member_gender',\n",
       " 'age',\n",
       " 'date']"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goBike.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDaily = goBike.groupBy('date').agg(f.avg('duration_sec').alias('avg_duration_sec'),\n",
    "                                       f.count('*').alias('n_trips'),f.countDistinct('bike_id').alias('n_bikes'),\n",
    "                                       f.sum(f.when(goBike.user_type == 'Subscriber',1).otherwise(0)).alias('N_subscriber'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['date', 'avg_duration_sec', 'n_trips', 'n_bikes', 'N_subscriber']"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataDaily.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_end = goBike.where(goBike['start_station_id'] <= goBike['end_station_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_start= goBike.where(goBike['start_station_id'] > goBike['end_station_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[end_station_id: int, start_station_id: int]"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_start.select('end_station_id','start_station_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_routes = start_end.select('start_station_id','end_station_id','date').union(end_start.select('end_station_id','start_station_id','date'))\\\n",
    "    .dropDuplicates().groupBy('date').agg(f.count('*').alias('n_routes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|    date|n_routes|\n",
      "+--------+--------+\n",
      "|21.12.17|    1540|\n",
      "|17.09.17|    1062|\n",
      "|04.08.17|    1000|\n",
      "|05.08.17|     635|\n",
      "|15.12.17|    1886|\n",
      "|17.12.17|     994|\n",
      "|07.07.17|     361|\n",
      "|03.12.17|    1051|\n",
      "|09.09.17|    1066|\n",
      "|13.12.17|    1963|\n",
      "|06.08.17|     583|\n",
      "|17.11.17|    1904|\n",
      "|20.09.17|    1740|\n",
      "|13.09.17|    1680|\n",
      "|27.10.17|    1961|\n",
      "|08.10.17|     958|\n",
      "|12.10.17|    1702|\n",
      "|12.12.17|    1967|\n",
      "|01.10.17|    1037|\n",
      "|08.12.17|    1892|\n",
      "+--------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unique_routes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+-------+-------+------------+--------+\n",
      "|    date|  avg_duration_sec|n_trips|n_bikes|N_subscriber|n_routes|\n",
      "+--------+------------------+-------+-------+------------+--------+\n",
      "|04.08.17| 860.2129807692307|   2080|    882|        1776|    1000|\n",
      "|17.09.17|1275.0027855153203|   1795|    953|        1226|    1062|\n",
      "|21.12.17| 669.1406695156695|   2808|   1284|        2671|    1540|\n",
      "|05.08.17|1264.4980952380952|   1050|    606|         748|     635|\n",
      "|15.12.17| 763.8888888888889|   3663|   1431|        3396|    1886|\n",
      "|07.07.17| 873.8014042126379|    997|    392|         932|     361|\n",
      "|17.12.17|1075.1111869031379|   1466|    876|        1275|     994|\n",
      "|03.12.17| 874.0664183736809|   1611|    923|        1375|    1051|\n",
      "|09.09.17|1329.9659284497445|   1761|    915|        1283|    1066|\n",
      "|13.12.17| 659.8088491175739|   4023|   1473|        3788|    1963|\n",
      "|06.08.17| 1210.444562899787|    938|    564|         667|     583|\n",
      "|13.09.17| 717.3768978862756|   3359|   1278|        3074|    1680|\n",
      "|17.11.17| 707.8836183121897|   3626|   1309|        3369|    1904|\n",
      "|20.09.17| 776.5570261898057|   3551|   1339|        3214|    1740|\n",
      "|27.10.17| 799.2955032119914|   3736|   1348|        3413|    1961|\n",
      "|08.10.17|1197.4256198347107|   1452|    856|        1052|     958|\n",
      "|12.10.17| 701.3281589568458|   3221|   1233|        2970|    1702|\n",
      "|12.12.17| 713.4284289276808|   4010|   1504|        3796|    1967|\n",
      "|01.10.17|           1090.08|   1650|    935|        1243|    1037|\n",
      "|03.07.17|  865.086046511628|    430|    243|         372|     233|\n",
      "+--------+------------------+-------+-------+------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataDaily = dataDaily.join(unique_routes,'date')\n",
    "dataDaily.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Przygotowanie danych**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "geo_id = [random.choice([\"regA\",\"regB\",\"regC\",\"regD\",\"regE\",\"regF\"]) for x in range(400)]\n",
    "prod_id = [random.choice([\"prodA\",\"prodB\",\"prodC\",\"prodD\",\"prodE\",\"prodF\",\n",
    "                          \"prodG\",\"prodH\",\"prodI\",\"prodJ\",\"prodK\",\"prodL\",\"prodM\"]) for x in range(400)]\n",
    "date = [random.choice([\"2015-\",\"2016-\",\"2017-\"]) + \n",
    "        random.choice([\"01-\",\"02-\",\"03-\",\"04-\",\"05-\",\"06-\",\"07-\",\"08-\",\"09-\",\"10-\",\"11-\",\"12-\"]) + \"01\"\n",
    "        for x in range(400)]\n",
    "value = [random.uniform(10000,100000) for x in range(400)]\n",
    "volume = [random.uniform(1000,10000) for x in range(400)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([Row(prod=p, geo=g, val=v, vol=vl, dt=d) \n",
    "                            for p,g,v,vl,d in zip(prod_id, geo_id, value, volume, date)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dt: string (nullable = true)\n",
      " |-- geo: string (nullable = true)\n",
      " |-- prod: string (nullable = true)\n",
      " |-- val: double (nullable = true)\n",
      " |-- vol: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"dt\", f.to_date(df[\"dt\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dt: date (nullable = true)\n",
      " |-- geo: string (nullable = true)\n",
      " |-- prod: string (nullable = true)\n",
      " |-- val: double (nullable = true)\n",
      " |-- vol: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----+------------------+------------------+\n",
      "|        dt| geo| prod|               val|               vol|\n",
      "+----------+----+-----+------------------+------------------+\n",
      "|2016-06-01|regF|prodB| 95087.40786730956| 5119.116197026179|\n",
      "|2016-05-01|regA|prodD|13902.853320925453| 3640.983140575281|\n",
      "|2016-05-01|regA|prodF| 80490.42963823416| 1394.256480932108|\n",
      "|2015-08-01|regF|prodE| 88028.28169838544| 2795.228503436695|\n",
      "|2015-12-01|regC|prodC| 56930.60932417757| 1377.153477373438|\n",
      "|2017-01-01|regB|prodH|51223.826988791385| 9400.338819553575|\n",
      "|2016-04-01|regB|prodI| 96762.35648098258| 5638.452303290489|\n",
      "|2017-02-01|regB|prodL|15474.286674505525|  9902.10432066511|\n",
      "|2017-01-01|regF|prodE|  53108.3719898527| 5887.276278887673|\n",
      "|2015-04-01|regA|prodJ| 46145.55290613044|3279.8238868235567|\n",
      "|2015-01-01|regF|prodM| 71748.77464560093| 7779.618269369784|\n",
      "|2017-03-01|regF|prodK|54124.196872980276| 2719.930876605198|\n",
      "|2015-03-01|regE|prodI| 91873.07462037064| 4212.767584318271|\n",
      "|2016-11-01|regA|prodA|16614.164418980545| 8027.574102805825|\n",
      "|2015-10-01|regE|prodK|17271.142966971274| 8792.184493702518|\n",
      "|2015-08-01|regD|prodI|  64746.7681270096| 3987.322177432101|\n",
      "|2017-05-01|regA|prodE|15911.400998810252|2120.2750741994505|\n",
      "|2016-03-01|regA|prodK| 34751.43996021579| 4312.172568832057|\n",
      "|2017-10-01|regA|prodB| 66976.90518709138| 9005.378653110532|\n",
      "|2017-12-01|regB|prodC|59352.079064352445|7689.7693496765905|\n",
      "+----------+----+-----+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **ZADANIE 3:**\n",
    "1. Oblicz (globalną) średnią cenę produktów\n",
    "2. Oblicz wartość całkowitą dla regionów per miesiąc\n",
    "3. Oblicz udział wolumenu kombinacji region-produkt w całkowitym wolumenie produktu (jaka część całkowitego wolumenu danego produktu generowana jest w danym regionie)\n",
    "5. Stwórz kolumnę `flag` zwierającą wartość `True` gdy nazwy regionu i produktu kończą się na tą samą literę - wartość `False` w każdym innym przypadku (nadpisz df aby zawierał nową kolumnę)\n",
    "6. Oblicz iloczyn wartości i wolumenu gdy kolumna `flag` ma wartość `True`, w przeciwnym przypadku zwróć wartość 0 \n",
    "7. Stwórz kolumnę z rokiem wyciągniętym z daty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+------------------+------------------+\n",
      "| prod|           sum_val|           sum_vol|           avg_val|\n",
      "+-----+------------------+------------------+------------------+\n",
      "|prodE|1951480.6743014117| 184158.5712438623|10.596740956017008|\n",
      "|prodM|1823820.0376677099|202986.63483985714| 8.984926712571898|\n",
      "|prodL|  1797693.02778563|184257.72584209952| 9.756405163310063|\n",
      "|prodI| 1787783.648304336|174413.81151260604|10.250241267017556|\n",
      "|prodB|1371597.8375560446| 157216.1869349042|  8.72427874188272|\n",
      "|prodG| 2257371.517211618| 262849.8421537546| 8.588064952655225|\n",
      "|prodC| 1722946.964618383|163359.19703885887|10.546984778631955|\n",
      "|prodJ|1248958.8517933392|148945.79296129555| 8.385324801472498|\n",
      "|prodA|1550799.9084993473|156561.38992964628| 9.905379028611254|\n",
      "|prodD|1509002.4397189445|178922.07052249167| 8.433852991485772|\n",
      "|prodF| 1439529.328777088|136084.36978225425|10.578212112680163|\n",
      "|prodK|1711504.8470177962|183268.55596059817| 9.338780665602894|\n",
      "|prodH| 1519910.340914309|164459.17723464363|  9.24187002799949|\n",
      "+-----+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1\n",
    "df_sum = df.groupBy('prod').agg(f.sum('val').alias('sum_val'),f.sum('vol').alias('sum_vol'))\n",
    "df_sum.withColumn('avg_val',df_sum['sum_val']/df_sum['sum_vol']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+------------------+\n",
      "| geo|        dt|          sum(val)|\n",
      "+----+----------+------------------+\n",
      "|regA|2015-01-01| 76166.82863310802|\n",
      "|regA|2015-02-01|155261.15709983977|\n",
      "|regA|2015-03-01|163084.09896991577|\n",
      "|regA|2015-04-01| 97789.60220958495|\n",
      "|regA|2015-05-01|  148582.374710774|\n",
      "|regA|2015-06-01|123526.96146645347|\n",
      "|regA|2015-07-01|121993.10654308298|\n",
      "|regA|2015-08-01|  99556.6117479058|\n",
      "|regA|2015-09-01| 91570.84274473095|\n",
      "|regA|2015-10-01|210166.80806373555|\n",
      "|regA|2015-11-01| 57471.38096153344|\n",
      "|regA|2015-12-01|111841.62910877481|\n",
      "|regA|2016-02-01| 50814.90126167026|\n",
      "|regA|2016-03-01|113431.95468646797|\n",
      "|regA|2016-04-01| 39926.67475347864|\n",
      "|regA|2016-05-01|247362.35939429235|\n",
      "|regA|2016-06-01|196490.67133782775|\n",
      "|regA|2016-07-01| 76491.63986398441|\n",
      "|regA|2016-08-01|100456.22453586152|\n",
      "|regA|2016-09-01|111773.61490563425|\n",
      "+----+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#2\n",
    "df.groupBy('geo','dt').sum('val').orderBy('geo','dt').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "windowSpec = Window.partitionBy('prod')\n",
    "\n",
    "step1 = df.groupBy(['prod','geo']).agg(f.sum('vol').alias('prod_geo_vol'))\\\n",
    ".select('prod','geo','prod_geo_vol',f.sum('prod_geo_vol').over(windowSpec).alias('prod_vol'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------------------+------------------+\n",
      "| prod| geo|      prod_geo_vol|          prod_vol|\n",
      "+-----+----+------------------+------------------+\n",
      "|prodE|regF|45626.348915020906|184158.57124386227|\n",
      "|prodE|regB| 29266.72538672742|184158.57124386227|\n",
      "|prodE|regD|16189.072187292655|184158.57124386227|\n",
      "|prodE|regC| 9562.599738735049|184158.57124386227|\n",
      "|prodE|regA|56972.673496617695|184158.57124386227|\n",
      "|prodE|regE| 26541.15151946855|184158.57124386227|\n",
      "|prodM|regC| 7406.281936064914|202986.63483985714|\n",
      "|prodM|regD| 16226.56288030393|202986.63483985714|\n",
      "|prodM|regA| 60681.85156695515|202986.63483985714|\n",
      "|prodM|regB|63458.168881366684|202986.63483985714|\n",
      "|prodM|regE|10466.021919063438|202986.63483985714|\n",
      "|prodM|regF| 44747.74765610305|202986.63483985714|\n",
      "|prodL|regA|18254.985976197902|184257.72584209955|\n",
      "|prodL|regE| 19839.61645806826|184257.72584209955|\n",
      "|prodL|regD|53726.753410583726|184257.72584209955|\n",
      "|prodL|regF| 35923.04327550772|184257.72584209955|\n",
      "|prodL|regC|27077.076693677303|184257.72584209955|\n",
      "|prodL|regB|29436.250028064616|184257.72584209955|\n",
      "|prodI|regB|39443.033509682544|174413.81151260604|\n",
      "|prodI|regC|21716.578131403385|174413.81151260604|\n",
      "+-----+----+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "step1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------------------+------------------+-------------------+\n",
      "| prod| geo|      prod_geo_vol|          prod_vol|              ratio|\n",
      "+-----+----+------------------+------------------+-------------------+\n",
      "|prodA|regA|  57461.9397984923|156561.38992964628| 0.3670249722764589|\n",
      "|prodA|regB| 8743.846246073634|156561.38992964628|0.05584931412529514|\n",
      "|prodA|regC| 14053.92990636366|156561.38992964628|0.08976625662737825|\n",
      "|prodA|regD| 37202.53450475852|156561.38992964628|0.23762266368148724|\n",
      "|prodA|regE| 33270.82691436357|156561.38992964628|0.21250978245220245|\n",
      "|prodA|regF| 5828.312559594595|156561.38992964628|0.03722701083717801|\n",
      "|prodB|regA| 15352.48052298814| 157216.1869349042|0.09765203457926937|\n",
      "|prodB|regB| 19723.26114147876| 157216.1869349042|0.12545311984729177|\n",
      "|prodB|regC| 35909.90666722029| 157216.1869349042|0.22841100122908392|\n",
      "|prodB|regD|16869.660292353863| 157216.1869349042|0.10730231168460276|\n",
      "|prodB|regE| 45142.00400298924| 157216.1869349042| 0.2871333091272619|\n",
      "|prodB|regF|24218.874307873913| 157216.1869349042|0.15404822353249037|\n",
      "|prodC|regA|27972.624689358832|163359.19703885887|0.17123385273927907|\n",
      "|prodC|regB| 43378.45631182626|163359.19703885887| 0.2655403374779546|\n",
      "|prodC|regC|19651.339032234744|163359.19703885887|0.12029527194333726|\n",
      "|prodC|regD|15769.338319242232|163359.19703885887| 0.0965316835849231|\n",
      "|prodC|regE| 21618.55338969844|163359.19703885887| 0.1323375345959613|\n",
      "|prodC|regF|34968.885296498374|163359.19703885887| 0.2140613196585448|\n",
      "|prodD|regA| 40815.86283495159|178922.07052249167|0.22812089484410908|\n",
      "|prodD|regB|25810.238836571694|178922.07052249167|0.14425408090349132|\n",
      "+-----+----+------------------+------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "step1.withColumn('ratio',step1['prod_geo_vol'] / step1['prod_vol']).orderBy('prod','geo').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4\n",
    "df = df.withColumn('flag',f.when(df.prod.substr(5,1) == df.geo.substr(4,1),'true').otherwise('false'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----+------------------+------------------+-----+\n",
      "|        dt| geo| prod|               val|               vol| flag|\n",
      "+----------+----+-----+------------------+------------------+-----+\n",
      "|2016-06-01|regF|prodB| 95087.40786730956| 5119.116197026179|false|\n",
      "|2016-05-01|regA|prodD|13902.853320925453| 3640.983140575281|false|\n",
      "|2016-05-01|regA|prodF| 80490.42963823416| 1394.256480932108|false|\n",
      "|2015-08-01|regF|prodE| 88028.28169838544| 2795.228503436695|false|\n",
      "|2015-12-01|regC|prodC| 56930.60932417757| 1377.153477373438| true|\n",
      "|2017-01-01|regB|prodH|51223.826988791385| 9400.338819553575|false|\n",
      "|2016-04-01|regB|prodI| 96762.35648098258| 5638.452303290489|false|\n",
      "|2017-02-01|regB|prodL|15474.286674505525|  9902.10432066511|false|\n",
      "|2017-01-01|regF|prodE|  53108.3719898527| 5887.276278887673|false|\n",
      "|2015-04-01|regA|prodJ| 46145.55290613044|3279.8238868235567|false|\n",
      "|2015-01-01|regF|prodM| 71748.77464560093| 7779.618269369784|false|\n",
      "|2017-03-01|regF|prodK|54124.196872980276| 2719.930876605198|false|\n",
      "|2015-03-01|regE|prodI| 91873.07462037064| 4212.767584318271|false|\n",
      "|2016-11-01|regA|prodA|16614.164418980545| 8027.574102805825| true|\n",
      "|2015-10-01|regE|prodK|17271.142966971274| 8792.184493702518|false|\n",
      "|2015-08-01|regD|prodI|  64746.7681270096| 3987.322177432101|false|\n",
      "|2017-05-01|regA|prodE|15911.400998810252|2120.2750741994505|false|\n",
      "|2016-03-01|regA|prodK| 34751.43996021579| 4312.172568832057|false|\n",
      "|2017-10-01|regA|prodB| 66976.90518709138| 9005.378653110532|false|\n",
      "|2017-12-01|regB|prodC|59352.079064352445|7689.7693496765905|false|\n",
      "+----------+----+-----+------------------+------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----+------------------+------------------+-----+-------------------+\n",
      "|        dt| geo| prod|               val|               vol| flag|            vol_val|\n",
      "+----------+----+-----+------------------+------------------+-----+-------------------+\n",
      "|2016-06-01|regF|prodB| 95087.40786730956| 5119.116197026179|false|                0.0|\n",
      "|2016-05-01|regA|prodD|13902.853320925453| 3640.983140575281|false|                0.0|\n",
      "|2016-05-01|regA|prodF| 80490.42963823416| 1394.256480932108|false|                0.0|\n",
      "|2015-08-01|regF|prodE| 88028.28169838544| 2795.228503436695|false|                0.0|\n",
      "|2015-12-01|regC|prodC| 56930.60932417757| 1377.153477373438| true| 7.84021865997798E7|\n",
      "|2017-01-01|regB|prodH|51223.826988791385| 9400.338819553575|false|                0.0|\n",
      "|2016-04-01|regB|prodI| 96762.35648098258| 5638.452303290489|false|                0.0|\n",
      "|2017-02-01|regB|prodL|15474.286674505525|  9902.10432066511|false|                0.0|\n",
      "|2017-01-01|regF|prodE|  53108.3719898527| 5887.276278887673|false|                0.0|\n",
      "|2015-04-01|regA|prodJ| 46145.55290613044|3279.8238868235567|false|                0.0|\n",
      "|2015-01-01|regF|prodM| 71748.77464560093| 7779.618269369784|false|                0.0|\n",
      "|2017-03-01|regF|prodK|54124.196872980276| 2719.930876605198|false|                0.0|\n",
      "|2015-03-01|regE|prodI| 91873.07462037064| 4212.767584318271|false|                0.0|\n",
      "|2016-11-01|regA|prodA|16614.164418980545| 8027.574102805825| true|1.333714360295662E8|\n",
      "|2015-10-01|regE|prodK|17271.142966971274| 8792.184493702518|false|                0.0|\n",
      "|2015-08-01|regD|prodI|  64746.7681270096| 3987.322177432101|false|                0.0|\n",
      "|2017-05-01|regA|prodE|15911.400998810252|2120.2750741994505|false|                0.0|\n",
      "|2016-03-01|regA|prodK| 34751.43996021579| 4312.172568832057|false|                0.0|\n",
      "|2017-10-01|regA|prodB| 66976.90518709138| 9005.378653110532|false|                0.0|\n",
      "|2017-12-01|regB|prodC|59352.079064352445|7689.7693496765905|false|                0.0|\n",
      "+----------+----+-----+------------------+------------------+-----+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#5\n",
    "df.withColumn('vol_val',f.when(df['flag']==True,df['val'] * df['vol']).otherwise(0)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----+------------------+------------------+-----+----+\n",
      "|        dt| geo| prod|               val|               vol| flag|year|\n",
      "+----------+----+-----+------------------+------------------+-----+----+\n",
      "|2016-06-01|regF|prodB| 95087.40786730956| 5119.116197026179|False|2016|\n",
      "|2016-05-01|regA|prodD|13902.853320925453| 3640.983140575281|False|2016|\n",
      "|2016-05-01|regA|prodF| 80490.42963823416| 1394.256480932108|False|2016|\n",
      "|2015-08-01|regF|prodE| 88028.28169838544| 2795.228503436695|False|2015|\n",
      "|2015-12-01|regC|prodC| 56930.60932417757| 1377.153477373438| True|2015|\n",
      "|2017-01-01|regB|prodH|51223.826988791385| 9400.338819553575|False|2017|\n",
      "|2016-04-01|regB|prodI| 96762.35648098258| 5638.452303290489|False|2016|\n",
      "|2017-02-01|regB|prodL|15474.286674505525|  9902.10432066511|False|2017|\n",
      "|2017-01-01|regF|prodE|  53108.3719898527| 5887.276278887673|False|2017|\n",
      "|2015-04-01|regA|prodJ| 46145.55290613044|3279.8238868235567|False|2015|\n",
      "|2015-01-01|regF|prodM| 71748.77464560093| 7779.618269369784|False|2015|\n",
      "|2017-03-01|regF|prodK|54124.196872980276| 2719.930876605198|False|2017|\n",
      "|2015-03-01|regE|prodI| 91873.07462037064| 4212.767584318271|False|2015|\n",
      "|2016-11-01|regA|prodA|16614.164418980545| 8027.574102805825| True|2016|\n",
      "|2015-10-01|regE|prodK|17271.142966971274| 8792.184493702518|False|2015|\n",
      "|2015-08-01|regD|prodI|  64746.7681270096| 3987.322177432101|False|2015|\n",
      "|2017-05-01|regA|prodE|15911.400998810252|2120.2750741994505|False|2017|\n",
      "|2016-03-01|regA|prodK| 34751.43996021579| 4312.172568832057|False|2016|\n",
      "|2017-10-01|regA|prodB| 66976.90518709138| 9005.378653110532|False|2017|\n",
      "|2017-12-01|regB|prodC|59352.079064352445|7689.7693496765905|False|2017|\n",
      "+----------+----+-----+------------------+------------------+-----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#6\n",
    "df.withColumn('year',f.year('dt')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Zaawansowane operacje na oknach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ranking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSpec = Window.partitionBy(\"prod\").orderBy(\"dt\")\n",
    "\n",
    "df.withColumn(\"ranked\", f.rank().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"\"\"select dt, geo, prod, val, vol, flag, rank() over (partition by prod order by dt) as ranked from df\"\"\"\n",
    "spark.sql(q).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Różnica od pierwszej wartości**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSpec = Window.partitionBy(\"prod\").orderBy(\"dt\")\n",
    "\n",
    "df.withColumn(\"diff_from_first\", df.val - f.first(df.val).over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"\"\"select dt, geo, prod, val, vol, flag, val - first(val) over (partition by prod order by dt) as diff_from_first from df\"\"\"\n",
    "spark.sql(q).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Średnia ruchoma**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSpec = Window.partitionBy(\"prod\").orderBy(\"dt\").rowsBetween(-1,1)\n",
    "\n",
    "df.withColumn(\"moving_avg\", f.avg(df.val).over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"\"\"select dt, geo, prod, val, vol, flag, \\\n",
    "avg(val) over (partition by prod order by dt rows between 1 preceding and 1 following) as moving_avg from df\"\"\"\n",
    "spark.sql(q).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Średnia od pierwszego do bierzącego rekordu**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSpec = Window.partitionBy(\"prod\").orderBy(\"dt\")\n",
    "\n",
    "df.withColumn(\"avg_from_start\", f.avg(df.val).over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"\"\"select dt, geo, prod, val, vol, flag, avg(val) over (partition by prod order by dt) as avg_from_start from df\"\"\"\n",
    "spark.sql(q).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inne podejście\n",
    "windowSpec = Window.partitionBy(\"prod\").orderBy(\"dt\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "df.withColumn(\"avg_from_start\", f.avg(df.val).over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"\"\"select dt, geo, prod, val, vol, flag, \\\n",
    "avg(val) over (partition by prod order by dt rows between unbounded preceding and current row) as avg_from_start from df\"\"\"\n",
    "spark.sql(q).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **ZADANIE 4:**\n",
    "8. Pogrupuj dane po dacie i produkcie po czym porównaj wolumen produktów ze średnim wolumenem z trzech wcześniejszych okresów\n",
    "9. Stwórz kolumnę z rankingiem opartm na dacie dla kombinacji produkt-region\n",
    "10. Oblicz różnicę w wolumenie pomiędzy następującymi po sobie datami dla regionów\n",
    "11. Oblicz różnicę rok do roku w wartości i wolumenie dla produktów\n",
    "12. Oblicz udział w całkowitej wartości poszczególnych produktów w danym roku dla każdego regionu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dodatek:**\n",
    "\n",
    "#### User Defined Functions (UDFs)\n",
    "\n",
    "(Używaj tylko w ostateczności)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, StringType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def udfPower3(value):\n",
    "    return(value**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udfPower3(3.14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "power3 = f.udf(udfPower3, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(power3(df.val)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.udf.register(\"power3\", udfPower3, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select power3(val) from df\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def udfDuplicate_string(x):\n",
    "    return x + \" \" + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udfDuplicate_string(\"input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "duplicate_string = f.udf(udfDuplicate_string, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(duplicate_string(df.prod)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.udf.register(\"duplicate_string\", udfDuplicate_string, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select duplicate_string(prod) from df\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def udfTwo_col(x,y):\n",
    "    return x / y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udfTwo_col(10,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "two_col = f.udf(udfTwo_col, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(two_col(df.vol,df.val)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.udf.register(\"two_col\", udfTwo_col, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select two_col(vol,val) from df\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
